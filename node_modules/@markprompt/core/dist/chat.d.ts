import type { BaseOptions, Chat, ChatCompletionMessage, ChatCompletionMessageParam, ChatCompletionMetadata, ChatCompletionTool, ChatCompletionToolChoiceOption, OpenAIModelId } from './types.js';
export type { ChatCompletionAssistantMessageParam, ChatCompletionFunctionMessageParam, ChatCompletionMessageParam, ChatCompletionSystemMessageParam, ChatCompletionToolMessageParam, ChatCompletionUserMessageParam, } from 'openai/resources/index.mjs';
export interface ChatMessage {
    role: 'user' | 'assistant';
    content: string;
}
export interface PoliciesOptions {
    /**
     * If true, enable the use of policies.
     * @default true
     **/
    enabled?: boolean;
    /**
     * If true, use all policies added in the project.
     * Otherwise, only use the ones excplicitly specified
     * in the `ids` list.
     * @default true
     **/
    useAll?: boolean;
    /**
     * Only use specific policies for retrieval.
     * @default []
     **/
    ids?: string[];
}
export interface RetrievalOptions {
    /**
     * If true, enable retrieval.
     * @default true
     **/
    enabled?: boolean;
    /**
     * If true, use all sources connected in the project.
     * Otherwise, only use the ones excplicitly specified
     * in the `ids` list.
     * @default true
     **/
    useAll?: boolean;
    /**
     * Only use specific sources for retrieval.
     * @default []
     **/
    ids?: string[];
}
export interface SubmitChatOptions {
    /**
     * The assistant ID.
     **/
    assistantId?: string;
    /**
     * The assistant version ID. If not provided, the default version of
     * the assistant will be used.
     **/
    assistantVersionId?: string;
    /**
     * The system prompt.
     **/
    systemPrompt?: string;
    /**
     * Context to use for template variable replacements in the system prompt.
     * @default {}
     **/
    context?: any;
    /**
     * The OpenAI model to use.
     * @default "gpt-4o"
     **/
    model?: OpenAIModelId;
    /**
     * Options for the use of policies.
     **/
    policiesOptions?: PoliciesOptions;
    /**
     * Options for retrieval.
     **/
    retrievalOptions?: RetrievalOptions;
    /**
     * The output format of the response.
     * @default "markdown"
     */
    outputFormat?: 'markdown' | 'slack' | 'html';
    /**
     * If true, output the response in JSON format.
     * @default false
     */
    jsonOutput?: boolean;
    /**
     * Remove PII from chat messages.
     * @default false
     */
    redact?: boolean;
    /**
     * The model temperature.
     * @default 0.1
     **/
    temperature?: number;
    /**
     * The model top P.
     * @default 1
     **/
    topP?: number;
    /**
     * The model frequency penalty.
     * @default 0
     **/
    frequencyPenalty?: number;
    /**
     * The model present penalty.
     * @default 0
     **/
    presencePenalty?: number;
    /**
     * The max number of tokens to include in the response.
     * @default 500
     * */
    maxTokens?: number;
    /**
     * The number of sections to include in the prompt context.
     * @default 10
     * */
    sectionsMatchCount?: number;
    /**
     * The similarity threshold between the input question and selected sections.
     * @default 0.5
     * */
    sectionsMatchThreshold?: number;
    /**
     * Thread ID. Returned with the first, and every subsequent, chat response. Used to continue a thread.
     * @default undefined
     */
    threadId?: string;
    /**
     * A list of tools the model may call. Currently, only functions are
     * supported as a tool. Use this to provide a list of functions the model may
     * generate JSON inputs for.
     */
    tools?: ChatCompletionTool[];
    /**
     * Controls which (if any) function is called by the model. `none` means the
     * model will not call a function and instead generates a message. `auto`
     * means the model can pick between generating a message or calling a
     * function. Specifying a particular function via
     * `{"type: "function", "function": {"name": "my_function"}}` forces the
     * model to call that function. `none` is the default when no functions are present. `auto` is the default if functions are present.
     */
    toolChoice?: ChatCompletionToolChoiceOption;
    /**
     * Whether or not to inject context relevant to the query.
     * @default false
     **/
    doNotInjectContext?: boolean;
    /**
     * If true, the bot may encourage the user to ask a follow-up question, for instance to gather additional information.
     * @default true
     **/
    allowFollowUpQuestions?: boolean;
    /**
     * Whether or not to include message in insights.
     * @default false
     **/
    excludeFromInsights?: boolean;
    /**
     * AbortController signal.
     * @default undefined
     **/
    signal?: AbortSignal;
    /**
     * Enabled debug mode. This will log debug and error information to the console.
     * @default false
     */
    debug?: boolean;
    /**
     * Message returned when the model does not have an answer.
     * @default "Sorry, I am not sure how to answer that."
     * @deprecated Will be removed.
     **/
    iDontKnowMessage?: string;
    /**
     * Disable streaming and return the entire response at once.
     */
    stream?: boolean;
}
export declare const DEFAULT_SUBMIT_CHAT_OPTIONS: {
    readonly frequencyPenalty: 0;
    readonly iDontKnowMessage: "Sorry, I am not sure how to answer that.";
    readonly model: "gpt-4o";
    readonly presencePenalty: 0;
    readonly temperature: 0.1;
    readonly topP: 1;
    readonly stream: true;
    readonly outputFormat: "markdown";
};
export type SubmitChatYield = Chat.Completions.ChatCompletionChunk.Choice.Delta & ChatCompletionMetadata;
export type SubmitChatReturn = ChatCompletionMessage & ChatCompletionMetadata;
export declare function submitChat(messages: ChatCompletionMessageParam[], projectKey: string, options?: SubmitChatOptions & BaseOptions): AsyncGenerator<SubmitChatYield, SubmitChatReturn | undefined>;
//# sourceMappingURL=chat.d.ts.map